{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchically Deep Convolutional Neural Network For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import randint\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of coarse categories\n",
    "coarse_categories = 20\n",
    "\n",
    "# The number of fine categories\n",
    "fine_categories = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Cifar100 Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y_c), (x_test, y_c_test) = cifar100.load_data(label_mode='coarse')\n",
    "(X, y), (x_test, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-To-Coarse Mapping**\n",
    "\n",
    "(Ideally, this would be done through spectral clustering as opposed to hard-coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fine2coarse = np.zeros((fine_categories,coarse_categories))\n",
    "for i in range(coarse_categories):\n",
    "    index = np.where(y_c_test[:,0] == i)[0]\n",
    "    fine_cat = np.unique([y_test[j,0] for j in index])\n",
    "    for j in fine_cat:\n",
    "        fine2coarse[j,i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_c = 0; # Clear y_c in interest of saving mem\n",
    "y_c_test=0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: One Hot Encoding\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function extends a matrix to one-hot encoding\n",
    "#    \n",
    "#    Parameters:\n",
    "#        y    Array of label values\n",
    "# \n",
    "#    Returns:\n",
    "#        y_new    One hot encoded array of labels\n",
    "################################################################################\n",
    "def one_hot(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    y_new = np.eye(n_values)[y[:,0]]\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=one_hot(y)\n",
    "y_test=one_hot(y_test)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply ZCA Whitening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: ZCA\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function applies ZCA Whitening to the image set\n",
    "#    \n",
    "#    Parameters:\n",
    "#        x_1           Array of MxNxC images to compute the ZCA Whitening\n",
    "#        x_2           Array of MxNxC images to apply the ZCA transform\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        An array of MxNxC zca whitened images\n",
    "################################################################################\n",
    "def zca(x_1, x_2, epsilon=1e-5):\n",
    "        \n",
    "    with tf.name_scope('ZCA'):\n",
    "        \n",
    "        x1 = tf.placeholder(tf.float64, shape=np.shape(x_1), name='placeholder_x1')\n",
    "        x2 = tf.placeholder(tf.float64, shape=np.shape(x_2), name='placeholder_x2')\n",
    "        \n",
    "        flatx = tf.cast(tf.reshape(x1, (-1, np.prod(x_1.shape[-3:])),name=\"reshape_flat\"),tf.float64,name=\"flatx\")\n",
    "        sigma = tf.tensordot(tf.transpose(flatx),flatx, 1,name=\"sigma\") / tf.cast(tf.shape(flatx)[0],tf.float64) ### N-1 or N?\n",
    "        s, u, v = tf.svd(sigma,name=\"svd\")\n",
    "        pc = tf.tensordot(tf.tensordot(u,tf.diag(1. / tf.sqrt(s+epsilon)),1,name=\"inner_dot\"),tf.transpose(u),1, name=\"pc\")\n",
    "        \n",
    "        net1 = tf.tensordot(flatx, pc,1,name=\"whiten1\")\n",
    "        net1 = tf.reshape(net1,np.shape(x_1), name=\"output1\")\n",
    "        \n",
    "        flatx2 = tf.cast(tf.reshape(x2, (-1, np.prod(x_2.shape[-3:])),name=\"reshape_flat2\"),tf.float64,name=\"flatx2\")\n",
    "        net2 = tf.tensordot(flatx2, pc,1,name=\"whiten2\")\n",
    "        net2 = tf.reshape(net2,np.shape(x_2), name=\"output2\")\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            x_1,x_2 = sess.run([net1,net2], feed_dict={x1: x_1, x2: x_2})    \n",
    "    return x_1,x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "X,x_test = zca(X,x_test)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - ZCA Whitening: '+str(time2-time1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training set into Training and Validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.1, random_state=0)\n",
    "X = 0\n",
    "y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flip, pad and randomly crop each photo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#    Title: Preprocess Img\n",
    "################################################################################\n",
    "#    Description: \n",
    "#        This function pads images by 4 pixels, randomly crops them, then\n",
    "#        randomly flips them\n",
    "#    \n",
    "#    Parameters:\n",
    "#        x_1           Array of MxNxC images to compute the ZCA Whitening\n",
    "#        x_2           Array of MxNxC images to apply the ZCA transform\n",
    "#        num_batch    Number of batches to do the computation\n",
    "# \n",
    "#    Returns:\n",
    "#        An array of MxNxC zca whitened images\n",
    "################################################################################\n",
    "def preprocess_img(X):\n",
    "        \n",
    "    with tf.name_scope('Preproc'):\n",
    "        \n",
    "        images = tf.placeholder(tf.float64, shape=np.shape(X))\n",
    "        \n",
    "        net = tf.image.resize_image_with_crop_or_pad(images,40,40)\n",
    "        net = tf.map_fn(lambda img: tf.random_crop(img, [32,32,3]), net)\n",
    "        net = tf.map_fn(lambda img: tf.image.random_flip_left_right(img), net)\n",
    "        net = tf.map_fn(lambda img: tf.image.random_flip_up_down(img), net)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            x_t = sess.run(net, feed_dict={images: X})    \n",
    "    return x_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = preprocess_img(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import Input, Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "in_layer = Input(shape=(32, 32, 3), dtype='float32', name='main_input')\n",
    "\n",
    "net = Conv2D(384, 3, strides=1, padding='same', activation='elu')(in_layer)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "net = Conv2D(384, 1, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(384, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(640, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(640, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.2)(net)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "net = Conv2D(640, 1, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(768, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.3)(net)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "net = Conv2D(768, 1, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(896, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(896, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.4)(net)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "net = Conv2D(896, 3, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(1024, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(1024, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.5)(net)\n",
    "net = MaxPooling2D((2, 2), padding='valid')(net)\n",
    "\n",
    "net = Conv2D(1024, 1, strides=1, padding='same', activation='elu')(net)\n",
    "net = Conv2D(1152, 2, strides=1, padding='same', activation='elu')(net)\n",
    "net = Dropout(.6)(net)\n",
    "net = MaxPooling2D((2, 2), padding='same')(net)\n",
    "\n",
    "net = Flatten()(net)\n",
    "net = Dense(1152, activation='elu')(net)\n",
    "net = Dense(100, activation='softmax')(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=in_layer,outputs=net)\n",
    "sgd_coarse = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer= sgd_coarse, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tbCallBack = kr.callbacks.TensorBoard(log_dir='./data/graph/elu_sgd_rand/', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= 0\n",
    "step = 10\n",
    "stop = 30\n",
    "\n",
    "while index < stop:\n",
    "    model.fit(x_train, y_train, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val), callbacks=[tbCallBack])\n",
    "    index += step\n",
    "    model.save_weights('data/models/model_coarse'+str(index))\n",
    "save_index = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Most Recent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_fine = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_index = 15\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "    if i<trainable_index:\n",
    "        model.layers[i].trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning for Coarse Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_c = np.dot(y_train,fine2coarse)\n",
    "y_val_c = np.dot(y_val,fine2coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_coarse = Dense(20, activation='softmax')(model.layers[-2].output)\n",
    "\n",
    "model_c = Model(inputs=in_layer,outputs=out_coarse)\n",
    "model_c.compile(optimizer= sgd_fine, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_c.load_weights('data/models/model_coarse'+str(save_index),by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 10\n",
    "stop = 40\n",
    "\n",
    "while index < stop:\n",
    "    model_c.fit(x_train, y_train_c, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_val, y_val_c), callbacks=[tbCallBack])\n",
    "    index += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning for Fine Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Fine Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fine_model():\n",
    "    out_fine = Dense(fine_categories, activation='softmax')(model.layers[-2].output)\n",
    "    model_fine = Model(inputs=in_layer,outputs=out_fine)\n",
    "    model_fine.compile(optimizer= sgd_fine,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model_fine.load_weights('data/models/model_coarse'+str(save_index))\n",
    "    return model_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_models = {'models' : [{} for i in range(coarse_categories)]}\n",
    "for i in range(coarse_categories):\n",
    "    model_i = fine_model()\n",
    "    fine_models['models'][i] = model_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Fine Classifiers on Respective Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_error(y,yh):\n",
    "    # Threshold \n",
    "    yht = np.zeros(np.shape(yh))\n",
    "    yht[np.arange(len(yh)), yh.argmax(1)] = 1\n",
    "    # Evaluate Error\n",
    "    error = np.count_nonzero(np.count_nonzero(y-yht,1))/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(coarse_categories):\n",
    "    index= 0\n",
    "    step = 10\n",
    "    stop = 10\n",
    "    \n",
    "    # Get all training data for the coarse category\n",
    "    ind = np.where([(y_train[:,int(fine2coarse[i,j])]==1) for j in range(int(fine_categories/coarse_categories))])[1]\n",
    "    y_i = np.array([y_train[j] for j in ind])\n",
    "    x_i = np.array([x_train[j] for j in ind])\n",
    "    \n",
    "    # Get all validation data for the coarse category\n",
    "    indv = np.where([(y_val[:,int(fine2coarse[i,j])]==1) for j in range(int(fine_categories/coarse_categories))])[1]\n",
    "    y_iv = np.array([y_val[j] for j in indv])\n",
    "    x_iv = np.array([x_val[j] for j in indv])\n",
    "    \n",
    "    if (np.shape(x_i)[0]>0)&(np.shape(x_iv)[0]>0):\n",
    "        while index < stop:\n",
    "            fine_models['models'][i].fit(x_i, y_i, batch_size=batch, initial_epoch=index, epochs=index+step, validation_data=(x_iv, y_iv), verbose=0)\n",
    "            index += step\n",
    "        yh_f = fine_models['models'][i].predict(x_iv, batch_size=batch)\n",
    "        print('Fine Classifier '+str(i)+' Error: '+str(get_error(y_iv,yh_f))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Probabilistic Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_hdcnn(X, y):\n",
    "    yh = np.zeros(np.shape(y))\n",
    "\n",
    "    yh_s = model.predict(X, batch_size=batch)\n",
    "    print('Single Classification Error: '+str(get_error(y,yh_s)))\n",
    "    \n",
    "    yh_c = np.dot(yh_s, fine2coarse)\n",
    "    y_c = np.dot(y,fine2coarse)\n",
    "    \n",
    "    print('Coarse Classifier Error: '+str(get_error(y_c,yh_c)))\n",
    "\n",
    "    for i in range(coarse_categories):\n",
    "        if i%5 == 0:\n",
    "            print(\"Evaluating Fine Classifier: \", str(i))\n",
    "        yh_f = fine_models['models'][i].predict(X, batch_size=batch)\n",
    "        yh += np.multiply(yh_c[:,i].reshape((len(y)),1), yh_f)\n",
    "    \n",
    "    print('Overall Error: '+str(get_error(y,yh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_hdcnn(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
